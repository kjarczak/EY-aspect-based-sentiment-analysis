{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "damaged-customs",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 1.57MB/s]                    \n",
      "2021-01-27 15:39:51 INFO: Downloading these customized packages for language: pl (Polish)...\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | lfg     |\n",
      "| pos       | lfg     |\n",
      "| lemma     | lfg     |\n",
      "| depparse  | lfg     |\n",
      "| pretrain  | lfg     |\n",
      "=======================\n",
      "\n",
      "2021-01-27 15:39:51 INFO: File exists: /home/jarczi/stanza_resources/pl/tokenize/lfg.pt.\n",
      "2021-01-27 15:39:51 INFO: File exists: /home/jarczi/stanza_resources/pl/pos/lfg.pt.\n",
      "2021-01-27 15:39:51 INFO: File exists: /home/jarczi/stanza_resources/pl/lemma/lfg.pt.\n",
      "2021-01-27 15:39:51 INFO: File exists: /home/jarczi/stanza_resources/pl/depparse/lfg.pt.\n",
      "2021-01-27 15:39:51 INFO: File exists: /home/jarczi/stanza_resources/pl/pretrain/lfg.pt.\n",
      "2021-01-27 15:39:51 INFO: Finished downloading models and saved to /home/jarczi/stanza_resources.\n",
      "2021-01-27 15:39:51 INFO: Loading these models for language: pl (Polish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | lfg     |\n",
      "| pos       | lfg     |\n",
      "| lemma     | lfg     |\n",
      "| depparse  | lfg     |\n",
      "=======================\n",
      "\n",
      "/home/jarczi/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2021-01-27 15:39:51 INFO: Use device: cpu\n",
      "2021-01-27 15:39:51 INFO: Loading: tokenize\n",
      "2021-01-27 15:39:51 INFO: Loading: pos\n",
      "2021-01-27 15:39:52 INFO: Loading: lemma\n",
      "2021-01-27 15:39:52 INFO: Loading: depparse\n",
      "2021-01-27 15:39:53 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "stanza.download('pl', package='lfg')\n",
    "nlp = stanza.Pipeline('pl', package='lfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "saved-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(statement, sentiment_dictionary):\n",
    "    doc = nlp(statement)\n",
    "    sent_values = []\n",
    "    \n",
    "    for s in range(len(doc.sentences)):\n",
    "        sent_values.append(list())\n",
    "        for w in range(len(doc.sentences[s].words)):\n",
    "            val = sentiment_dictionary.sentiment[sentiment_dictionary.word == doc.sentences[s].words[w].lemma]\n",
    "            if(val.empty):\n",
    "                sent_values[s].append(None)\n",
    "            else:\n",
    "                sent_values[s].append(int(val.values[0]))\n",
    "    \n",
    "    sent_modificators = []  \n",
    "    for s in range(len(doc.sentences)):\n",
    "        sent_modificators.append([1]*len(doc.sentences[s].words))\n",
    "        for w in range(len(doc.sentences[s].words)):\n",
    "            word = doc.sentences[s].words[w]\n",
    "            \n",
    "            if(word.pos == 'PART' and word.feats != None and word.feats.split('|').count('Polarity=Neg')>0):\n",
    "                connected = [sub_word for sub_word in doc.sentences[s].words if sub_word.head == word.head]\n",
    "                connected.append(doc.sentences[s].words[word.head -1]) \n",
    "                \n",
    "                for sub_word in connected:\n",
    "                    if sub_word.deprel != 'conj':\n",
    "                        sent_modificators[s][sub_word.id-1] = -1 * sent_modificators[s][sub_word.id-1]\n",
    "                \n",
    "    norm = 0\n",
    "    weights = 0\n",
    "    for i in range(len(sent_values)):\n",
    "        konwn_vals = [x for x in sent_values[i] if x is not None]\n",
    "        n_known = len(konwn_vals)\n",
    "        sentence_weight =n_known/len(sent_values[i]) \n",
    "        weights += sentence_weight\n",
    "        \n",
    "        sent_values[i]= [val if val is not None else 0 for val in sent_values[i]]\n",
    "        sent_modificators[i]= [val if val is not None else 1 for val in sent_modificators[i]]\n",
    "        sent_values[i] = np.multiply(sent_modificators[i], sent_values[i])\n",
    "        \n",
    "        if n_known == 0:\n",
    "            n_known = 1\n",
    "        norm += sentence_weight * sum(sent_values[i])/n_known\n",
    "    \n",
    "    if weights == 0:\n",
    "            weights = 1\n",
    "    sentiment_norm = norm / weights\n",
    "        \n",
    "    return sentiment_norm    \n",
    "    if(sentiment_norm >= 0.33):\n",
    "        return 1\n",
    "    elif sentiment_norm <= -0.33:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "seasonal-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeEmoji(txt):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', txt)\n",
    "\n",
    "def removeUrl(txt):\n",
    "    urls_pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')                          \n",
    "    return urls_pattern.sub(r'', txt)\n",
    "\n",
    "def removeSymbols(txt):\n",
    "    symbols_pattern = re.compile('[^AaĄąBbCcĆćDdEeĘęFfGgHhIiJjKkLlŁłMmNnŃńOoÓóPpRrSsŚśTtUuWwVvXxYyZzŹźŻż#]+' u'')                          \n",
    "    return symbols_pattern.sub(r' ', txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "correct-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatise_strings_csv(input_file_name, output_file_name = 'output/output_'):\n",
    "    raw = pd.read_csv(input_file_name, encoding = 'cp1250')\n",
    "    sentiment_dictionary_adj = pd.DataFrame(columns=['word', 'sentiment'] )\n",
    "    sentiment_dictionary_noun = pd.DataFrame(columns=['word', 'sentiment'] )\n",
    "    sentiment_dictionary_verb = pd.DataFrame(columns=['word', 'sentiment'] )\n",
    "    sentiment_dictionary_else = pd.DataFrame(columns=['word', 'sentiment'] )\n",
    "\n",
    "    for i in range(len(raw['content']) ):\n",
    "        if(i%100 == 0):\n",
    "            print(\"Azure!: \"+ str(i/100))\n",
    "        raw_text = raw['content'][i]\n",
    "        text = removeEmoji(removeUrl(removeSymbols(raw_text)))\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        for sentence in doc.sentences:\n",
    "            for word in sentence.words:\n",
    "                if(word.pos == 'ADJ'):\n",
    "                    sentiment_dictionary_adj = sentiment_dictionary_adj.append({'word':word.lemma, 'sentiment':''}, ignore_index=True)\n",
    "                elif(word.pos == 'VERB'):\n",
    "                    sentiment_dictionary_verb = sentiment_dictionary_verb.append({'word':word.lemma, 'sentiment':''}, ignore_index=True)\n",
    "                elif(word.pos == 'NOUN'):\n",
    "                    sentiment_dictionary_noun = sentiment_dictionary_noun.append({'word':word.lemma, 'sentiment':''}, ignore_index=True)\n",
    "                else:\n",
    "                    sentiment_dictionary_else = sentiment_dictionary_else.append({'word':word.lemma, 'sentiment':''}, ignore_index=True)\n",
    "                    \n",
    "    sentiment_dictionary_adj = sentiment_dictionary_adj.drop_duplicates(subset=['word'])\n",
    "    sentiment_dictionary_verb = sentiment_dictionary_verb.drop_duplicates(subset=['word'])\n",
    "    sentiment_dictionary_noun = sentiment_dictionary_noun.drop_duplicates(subset=['word'])\n",
    "    sentiment_dictionary_else = sentiment_dictionary_else.drop_duplicates(subset=['word'])\n",
    "    \n",
    "    sentiment_dictionary_adj.to_csv(output_file_name+'adj.csv', index = False)\n",
    "    sentiment_dictionary_verb.to_csv(output_file_name+'verb.csv', index = False)\n",
    "    sentiment_dictionary_noun.to_csv(output_file_name+'noun.csv', index = False)\n",
    "    sentiment_dictionary_else.to_csv(output_file_name+'else.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-wisdom",
   "metadata": {},
   "source": [
    "***Tests***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = pd.read_csv(\"dictionary.csv\")\n",
    "tweets = list(pd.read_csv(\"tweets.csv\",encoding='cp1250').content)\n",
    "for text in tweets:\n",
    "    print(text)\n",
    "    print(\"Sentiment: \",get_sentiment(text,dictionary[dictionary['sentiment'] != 0]))\n",
    "    print(\"============================================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
