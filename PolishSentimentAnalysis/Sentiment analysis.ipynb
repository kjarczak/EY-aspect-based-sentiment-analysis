{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "surface-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "defined-people",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 2.53MB/s]                    \n",
      "2021-01-22 22:46:34 INFO: Downloading these customized packages for language: pl (Polish)...\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | lfg     |\n",
      "| pos       | lfg     |\n",
      "| lemma     | lfg     |\n",
      "| depparse  | lfg     |\n",
      "| pretrain  | lfg     |\n",
      "=======================\n",
      "\n",
      "2021-01-22 22:46:34 INFO: File exists: /home/jarczi/stanza_resources/pl/tokenize/lfg.pt.\n",
      "2021-01-22 22:46:34 INFO: File exists: /home/jarczi/stanza_resources/pl/pos/lfg.pt.\n",
      "2021-01-22 22:46:34 INFO: File exists: /home/jarczi/stanza_resources/pl/lemma/lfg.pt.\n",
      "2021-01-22 22:46:34 INFO: File exists: /home/jarczi/stanza_resources/pl/depparse/lfg.pt.\n",
      "2021-01-22 22:46:34 INFO: File exists: /home/jarczi/stanza_resources/pl/pretrain/lfg.pt.\n",
      "2021-01-22 22:46:34 INFO: Finished downloading models and saved to /home/jarczi/stanza_resources.\n",
      "2021-01-22 22:46:34 INFO: Loading these models for language: pl (Polish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | lfg     |\n",
      "| pos       | lfg     |\n",
      "| lemma     | lfg     |\n",
      "| depparse  | lfg     |\n",
      "=======================\n",
      "\n",
      "2021-01-22 22:46:34 INFO: Use device: cpu\n",
      "2021-01-22 22:46:34 INFO: Loading: tokenize\n",
      "2021-01-22 22:46:34 INFO: Loading: pos\n",
      "2021-01-22 22:46:35 INFO: Loading: lemma\n",
      "2021-01-22 22:46:35 INFO: Loading: depparse\n",
      "2021-01-22 22:46:36 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('pl', package='lfg')\n",
    "nlp = stanza.Pipeline('pl', package='lfg')\n",
    "sentiment_dict_file = 'nawl-analysis.csv'\n",
    "sentiment_dict_file2 = 'test_sentiment_dictionary.csv'\n",
    "sentiment_dictionary = pd.read_csv(sentiment_dict_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "other-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment1(statement):\n",
    "    doc = nlp(statement)\n",
    "    \n",
    "    sentence_sentiment = 0\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            sent = 0\n",
    "            val = sentiment_dictionary.category[sentiment_dictionary.word == word.lemma]\n",
    "            if(val.empty):\n",
    "                val = ''\n",
    "            else:    \n",
    "                val = val.values[0]\n",
    "\n",
    "            if(val == 'H'):\n",
    "                sentence_sentiment +=1 \n",
    "                sent = 1\n",
    "            elif(val == 'A' or val == 'S' or val == 'F' or val == 'D'):\n",
    "                sentence_sentiment -= 1\n",
    "                sent == -1\n",
    "\n",
    "            print(word.text, sent)\n",
    "\n",
    "    print(\"Sentyment zdania to: \"+str(sentence_sentiment))\n",
    "    \n",
    "def get_sentiment(statement):\n",
    "    doc = nlp(statement)\n",
    "    \n",
    "    text_sentiment = 0\n",
    "    for sentence in doc.sentences:\n",
    "        sentence_sentiment = 0\n",
    "        for word in sentence.words:\n",
    "            sent = 0\n",
    "            val = sentiment_dictionary.sentiment[sentiment_dictionary.word == word.lemma]\n",
    "            if(val.empty):\n",
    "                sent = 0\n",
    "            else:    \n",
    "                sent = int(val.values[0])\n",
    "\n",
    "            sentence_sentiment += sent \n",
    "            print(word.text, sent)\n",
    "        text_sentiment += sentence_sentiment\n",
    "        \n",
    "    if(text_sentiment>=0.33):\n",
    "        return 1\n",
    "    elif text_sentiment<=-0.33:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-journey",
   "metadata": {},
   "source": [
    "**Informacje o zdaniu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "usual-adolescent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(statement):\n",
    "    doc = nlp(statement)\n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            print(word.text, word.lemma, word.pos)\n",
    "            \n",
    "    for sentence in doc.sentences:\n",
    "        print(sentence.ents)\n",
    "        print(sentence.dependencies)\n",
    "        \n",
    "    print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-disco",
   "metadata": {},
   "source": [
    "***Tests***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "significant-avatar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lody 0\n",
      "są 0\n",
      "złe -1\n",
      ". 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment(\"Lody są złe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "authorized-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatise csv and ovveride it. Input and output file has two columns ['word', 'sentiment']\n",
    "def lemmatise_csv(file):\n",
    "    sentiment_dictionary_raw = pd.read_csv(file)\n",
    "    sentiment_dictionary_lemma = pd.DataFrame(columns=['word', 'sentiment'] )\n",
    "    for i in range(len(sentiment_dictionary_raw['word']) ):\n",
    "        word = sentiment_dictionary_raw['word'][i]\n",
    "        lemma = nlp(word).sentences[0].words[0].lemma\n",
    "        sentiment = sentiment_dictionary_raw['sentiment'][i]\n",
    "        sentiment_dictionary_lemma = sentiment_dictionary_lemma.append({'word':lemma, 'sentiment':sentiment}, ignore_index=True)\n",
    "    sentiment_dictionary_lemma.to_csv(file)\n",
    "    return sentiment_dictionary_lemma"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
